{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f2324f5-f71c-4363-a67a-1c3fd7d31a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "# pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40844dcc-79c4-454d-80e2-5c8530b8d492",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, read_csv, concat\n",
    "import boto3\n",
    "import os\n",
    "from os import remove\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e29de-ddd1-48fc-bd7a-8c83001d9830",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c42e61a-b26b-4108-bf8f-f333330d5a5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pandas import DataFrame, read_csv, concat\n",
    "# import boto3\n",
    "# from os import remove\n",
    "\n",
    "def download_and_concat_s3_csvs(output_bucket_name, concatenated_csv):\n",
    "    \"\"\"\n",
    "    Downloads CSV files from the specified S3 bucket, concatenates them into a single DataFrame, and returns it.\n",
    "    \n",
    "    Parameters:\n",
    "    - output_bucket_name (str): The name of the S3 bucket containing the CSV files.\n",
    "    - concatenated_csv (str): The name of the concatenated output file to be saved locally.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The concatenated DataFrame.\n",
    "    \"\"\"\n",
    "    # Initialize S3 client\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # List files already in the output directory\n",
    "    written_output_files = s3.list_objects(Bucket=output_bucket_name, Prefix='csv/', Delimiter='/')\n",
    "    written_output_files = [i['Key'] for i in written_output_files.get('Contents', [])]\n",
    "\n",
    "    # Filter out empty keys or directories\n",
    "    csv_files = [key for key in written_output_files if key.endswith('.csv')]\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Process each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        local_filename = csv_file.split('/')[-1]  # Extract the file name\n",
    "        # Download the file locally\n",
    "        s3.download_file(output_bucket_name, csv_file, local_filename)\n",
    "        \n",
    "        # Load the CSV into a DataFrame\n",
    "        df = pd.read_csv(local_filename)\n",
    "        dataframes.append(df)\n",
    "        \n",
    "        # Clean up the local file\n",
    "        remove(local_filename)\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    concat_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Save the concatenated DataFrame locally if needed\n",
    "    concat_df.to_csv(concatenated_csv, index=False)\n",
    "\n",
    "    return concat_df\n",
    "\n",
    "# TODO: deal with this later\n",
    "# # Save the concatenated DataFrame to a new CSV file\n",
    "# final_df.to_csv(concatenated_csv, index=False)\n",
    "\n",
    "# # Optional: Upload the concatenated CSV back to S3\n",
    "# s3.upload_file(concatenated_csv, output_bucket_name, f'csv/{concatenated_csv}')\n",
    "\n",
    "# # Remove the local concatenated file\n",
    "# remove(concatenated_csv)\n",
    "\n",
    "# print(f\"Concatenated CSV saved to {concatenated_csv} and uploaded to bucket.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99083334-ae17-4fc9-aa7b-18b20854762e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# Take the datetime object and turn it into the other needed columns\n",
    "def extract_date_info(dt):\n",
    "    # Extract basic components\n",
    "    year = dt.year\n",
    "    month = dt.month\n",
    "    day = dt.day\n",
    "    hour = dt.hour\n",
    "    minute = dt.minute\n",
    "    weekday = dt.weekday()  # 0=Monday, 6=Sunday\n",
    "    is_weekend = weekday >= 5\n",
    "\n",
    "    # Calculate the week of the month\n",
    "    first_day_of_month = dt.replace(day=1)\n",
    "    week_of_month = (dt.day + first_day_of_month.weekday()) // 7 + 1\n",
    "\n",
    "    # Determine the season (Northern Hemisphere)\n",
    "    if month in (12, 1, 2):\n",
    "        season = \"Winter\"\n",
    "    elif month in (3, 4, 5):\n",
    "        season = \"Spring\"\n",
    "    elif month in (6, 7, 8):\n",
    "        season = \"Summer\"\n",
    "    else:\n",
    "        season = \"Fall\"\n",
    "\n",
    "    return year, month, day, hour, minute, is_weekend, week_of_month, season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f034d358-66e3-450d-a972-f9f9d2f386c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_expanded_df(data):\n",
    "    \"\"\"\n",
    "    Preprocess the expanded DataFrame for use with the AWS Random Cut Forest (RCF) model.\n",
    "    \n",
    "    This function:\n",
    "    - Removes `date_time`.\n",
    "    - Encodes and transforms categorical and cyclical features.\n",
    "    - Normalizes numerical features.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): Expanded DataFrame with columns:\n",
    "      ['n_cars', 'traffic_speed', 'date_time', 'camera_id', \n",
    "       'year', 'month', 'day', 'hour', 'minute', \n",
    "       'is_weekend', 'week_of_month', 'season'].\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed DataFrame ready for RCF.\n",
    "    \"\"\"\n",
    "    # Drop the `date_time` column\n",
    "    data = data.drop(columns=[\"date_time\"])\n",
    "    \n",
    "    # Encode `is_weekend` as integer (0 or 1)\n",
    "    data[\"is_weekend\"] = data[\"is_weekend\"].astype(int)\n",
    "    \n",
    "    # Map `season` to numeric values if it's of type object\n",
    "    if data[\"season\"].dtype == \"object\":\n",
    "        season_mapping = {\"Winter\": 0, \"Spring\": 1, \"Summer\": 2, \"Fall\": 3}\n",
    "        data[\"season\"] = data[\"season\"].map(season_mapping)\n",
    "        if data[\"season\"].isna().any():\n",
    "            raise ValueError(\"Unmapped season value detected in the data.\")\n",
    "    \n",
    "    # Define a function for cyclical transformations\n",
    "    def add_cyclical_features(data, column, max_value):\n",
    "        data[f\"{column}_sin\"] = np.sin(2 * np.pi * data[column] / max_value)\n",
    "        data[f\"{column}_cos\"] = np.cos(2 * np.pi * data[column] / max_value)\n",
    "        return data\n",
    "    \n",
    "    # Apply cyclical transformations\n",
    "    data = add_cyclical_features(data, \"month\", 12)          # Cyclical for months\n",
    "    data = add_cyclical_features(data, \"day\", 31)            # Cyclical for days\n",
    "    data = add_cyclical_features(data, \"hour\", 24)           # Cyclical for hours\n",
    "    data = add_cyclical_features(data, \"minute\", 60)         # Cyclical for minutes\n",
    "    data = add_cyclical_features(data, \"week_of_month\", 5)   # Cyclical for weeks of the month\n",
    "    data = add_cyclical_features(data, \"season\", 4)          # Cyclical for seasons\n",
    "    data = add_cyclical_features(data, \"year\", 10)           # Cyclical for years (assuming a 10-year cycle, adjust as needed)\n",
    "    \n",
    "    # Drop the original cyclical columns\n",
    "    columns_to_drop = [\"month\", \"day\", \"hour\", \"minute\", \"week_of_month\", \"season\", \"year\"]\n",
    "    data = data.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    numerical_columns = [\n",
    "        \"n_cars\", \"traffic_speed\", \"camera_id\", \"is_weekend\"\n",
    "    ]\n",
    "    scaler = StandardScaler()\n",
    "    data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9355432b-c911-45ba-be21-dea4f9214b63",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33881baf-aa9e-43c2-b6ac-c3e54e22b0bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_preprocessed_data_to_local(data, filename='data.csv'):\n",
    "    \"\"\"\n",
    "    Saves the preprocessed data to a local CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): Preprocessed traffic data.\n",
    "    - filename (str): The name of the local file to be saved (default is 'data.csv').\n",
    "    \"\"\"\n",
    "    # Save the preprocessed data to CSV\n",
    "    data.to_csv(filename, header=False, index=False)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "406b5399-1687-4594-b413-07f012129fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_to_s3(local_file_path, bucket_name, s3_key):\n",
    "    \"\"\"\n",
    "    Uploads a local file to S3.\n",
    "\n",
    "    Parameters:\n",
    "    - local_file_path (str): The local file path of the CSV to be uploaded.\n",
    "    - bucket_name (str): The name of the S3 bucket.\n",
    "    - s3_key (str): The S3 object key (path where the file will be stored).\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_client.upload_file(local_file_path, bucket_name, s3_key)\n",
    "    \n",
    "    # Return the full S3 URI\n",
    "    return f\"s3://{bucket_name}/{s3_key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c643b1da-4d58-4559-b267-9934b81e67f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import RandomCutForest\n",
    "import pandas as pd\n",
    "\n",
    "def train_rcf_model(input_data, bucket_name, output_data_file_path, region_name='us-east-1'):\n",
    "    \"\"\"\n",
    "    Trains the RCF model using the preprocessed data stored locally or in S3.\n",
    "\n",
    "    Parameters:\n",
    "    - input_data (str): Local file path or S3 URI of the preprocessed data.\n",
    "    - bucket_name (str): The name of the S3 bucket for output data.\n",
    "    - output_data_file_path (str): The path in the S3 bucket where model output will be saved.\n",
    "    - region_name (str): The AWS region for the SageMaker model.\n",
    "\n",
    "    Returns:\n",
    "    - sagemaker.estimator.Estimator: The trained RCF model.\n",
    "    \"\"\"\n",
    "    # Get the SageMaker session and execution role\n",
    "    session = sagemaker.Session()\n",
    "    role = get_execution_role()\n",
    "\n",
    "    # Upload the input data to S3 if it's a local file\n",
    "    if input_data.startswith('s3://'):\n",
    "        input_data_location = input_data  # Data is already in S3\n",
    "    else:\n",
    "        input_data_location = session.upload_data(path=input_data, bucket=bucket_name, key_prefix='rcf-clickstream')\n",
    "\n",
    "    # Initialize the RandomCutForest estimator\n",
    "    rcf = RandomCutForest(\n",
    "        role=role,\n",
    "        train_instance_count=1,\n",
    "        train_instance_type='ml.m4.xlarge',\n",
    "        data_location=input_data_location,\n",
    "        output_path=f's3://{bucket_name}/{output_data_file_path}',  # Output location\n",
    "        num_samples_per_tree=256,  # Hyperparameter\n",
    "        num_trees=100,  # Hyperparameter\n",
    "        sagemaker_session=session\n",
    "    )\n",
    "\n",
    "    # Load training data (assuming it's in CSV format)\n",
    "    train_data = pd.read_csv(input_data)  # Assuming input is a CSV file\n",
    "    \n",
    "    # Ensure all data is numeric (e.g., convert to float32)\n",
    "    train_data = train_data.apply(pd.to_numeric, errors='coerce')  # Convert all columns to numeric, non-convertible become NaN\n",
    "    train_data = train_data.fillna(0)  # Replace NaN values with 0, if needed (or handle accordingly)\n",
    "    \n",
    "    # Convert to numpy array with float32 dtype\n",
    "    train_data_numpy = train_data.to_numpy().astype('float32')\n",
    "\n",
    "    # Convert the data to the record set format\n",
    "    rcf_record_set = rcf.record_set(train_data_numpy, channel='train', encrypt=False)\n",
    "\n",
    "    # Train the model\n",
    "    rcf.fit(rcf_record_set)\n",
    "\n",
    "    return rcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2301896d-b160-40bf-96f6-9e7a174f3b86",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # SageMaker Training Function\n",
    "# def train_rcf_model1(input_data, bucket_name, output_data_file_path, region_name='us-east-1'):\n",
    "#     \"\"\"\n",
    "#     Trains the RCF model using the preprocessed data stored locally or in S3.\n",
    "\n",
    "#     Parameters:\n",
    "#     - input_data (str): Local file path or S3 URI of the preprocessed data.\n",
    "#     - region_name (str): The AWS region for the SageMaker model.\n",
    "\n",
    "#     Returns:\n",
    "#     - sagemaker.estimator.Estimator: The trained RCF model.\n",
    "#     \"\"\"\n",
    "#     # Get the RCF image URI\n",
    "#     rcf_image = sagemaker.image_uris.retrieve('randomcutforest', region=region_name)\n",
    "    \n",
    "#     # Get the execution role and session\n",
    "#     role = get_execution_role()\n",
    "#     session = Session()\n",
    "\n",
    "#     # Determine feature_dim based on input data (number of features in the data)\n",
    "#     # Assuming the input data is a CSV file, and we get the feature dimension\n",
    "#     # For example, reading the first row to determine the number of features.\n",
    "#     feature_dim = 0  # Replace this with the actual way to compute feature_dim based on input data\n",
    "#     if input_data.startswith('s3://'):\n",
    "#         # If the input data is an S3 URI, you might want to read the first row to compute feature_dim\n",
    "#         import boto3\n",
    "#         s3 = boto3.client('s3')\n",
    "#         bucket, key = input_data.replace('s3://', '').split('/', 1)\n",
    "#         obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "#         first_row = obj['Body'].read().decode('utf-8').splitlines()[0]  # Read the first row\n",
    "#         feature_dim = len(first_row.split(','))  # Assuming CSV format\n",
    "#     else:\n",
    "#         # If the input data is local, you can read the file directly\n",
    "#         with open(input_data, 'r') as f:\n",
    "#             first_row = f.readline()\n",
    "#             feature_dim = len(first_row.split(','))  # Assuming CSV format\n",
    "\n",
    "#     # Set hyperparameters\n",
    "#     num_samples_per_tree = 256\n",
    "#     num_trees = 100\n",
    "\n",
    "#     # Create SageMaker estimator for RCF\n",
    "#     rcf = sagemaker.estimator.Estimator(\n",
    "#         image_uri=rcf_image,\n",
    "#         role=role,\n",
    "#         instance_count=1,\n",
    "#         instance_type='ml.m4.xlarge',\n",
    "#         output_path=f's3://{bucket_name}/{output_data_file_path}',  # Saving output to S3\n",
    "#         sagemaker_session=session,\n",
    "#         hyperparameters={'feature_dim': feature_dim, 'num_samples_per_tree': num_samples_per_tree, 'num_trees': num_trees}\n",
    "#     )\n",
    "\n",
    "#     # Configure input data and distribution type\n",
    "#     train_input = TrainingInput(\n",
    "#         s3_data=input_data,\n",
    "#         content_type='csv'  # Assuming CSV format; modify if your data is in a different format\n",
    "#     )\n",
    "\n",
    "#     # Train the model\n",
    "#     rcf.fit({'train': train_input})\n",
    "    \n",
    "#     return rcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9528573e-125e-442f-8694-9850142a50d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# import sagemaker\n",
    "# from sagemaker import get_execution_role\n",
    "# from sagemaker.predictor import Predictor\n",
    "# from sagemaker import RandomCutForest\n",
    "# from sagemaker import Session\n",
    "\n",
    "# # WARNING Duplicate\n",
    "# # Deploy and predict\n",
    "# def deploy_and_predict(rcf_model, inference_data):\n",
    "#     \"\"\"\n",
    "#     Deploys the RCF model and makes a prediction.\n",
    "\n",
    "#     Parameters:\n",
    "#     - rcf_model (sagemaker.estimator.Estimator): The trained RCF model.\n",
    "#     - inference_data (np.array): The data to be used for inference.\n",
    "\n",
    "#     Returns:\n",
    "#     - dict: The predicted anomaly scores.\n",
    "#     \"\"\"\n",
    "#     # Deploy the model\n",
    "#     rcf_predictor = rcf_model.deploy(\n",
    "#         initial_instance_count=1,\n",
    "#         instance_type='ml.t3.medium'\n",
    "#     )\n",
    "    \n",
    "#     # Prepare the payload for inference (convert to numpy array and list)\n",
    "#     payload = json.dumps(inference_data.tolist())\n",
    "\n",
    "#     # Make a prediction (send payload to the deployed model)\n",
    "#     response = rcf_predictor.predict(payload)\n",
    "    \n",
    "#     # The response needs to be decoded and parsed from the result\n",
    "#     result = json.loads(response)\n",
    "\n",
    "#     print(\"Anomaly scores:\", result)\n",
    "\n",
    "#     # Clean up the endpoint after inference\n",
    "#     rcf_predictor.delete_endpoint()\n",
    "\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b234ab-4abe-4292-91c7-bf9ce65c7dd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a15d5e1b-107b-487a-a549-1575e4df3f64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   n_cars  traffic_speed\n",
      "0    18.0      13.292431\n",
      "1    19.0      10.643991\n",
      "2    20.0      14.672661\n",
      "3    17.0       7.979585\n",
      "4    17.0      10.490092\n"
     ]
    }
   ],
   "source": [
    "# Usage of the function\n",
    "input_bucket_name = 'jfrechmsml650output'\n",
    "concatenated_csv = 'concatenated_output.csv'\n",
    "output_bucket_name = 'dulcichmsml650bucket' # Make and enter bucket name\n",
    "output_path = 'output.csv' # ex. data/traffic_data.csv\n",
    "local_file_path = ''\n",
    "\n",
    "# Get the concatenated DataFrame\n",
    "concat_df = download_and_concat_s3_csvs(input_bucket_name, concatenated_csv)\n",
    "\n",
    "# Optionally, print the first few rows of the concatenated DataFrame\n",
    "print(concat_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "789e9c4c-a1ef-4dc4-9b42-67aff9c6dbe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_cars</th>\n",
       "      <th>traffic_speed</th>\n",
       "      <th>date_time</th>\n",
       "      <th>camera_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>13.292431</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.0</td>\n",
       "      <td>10.643991</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.0</td>\n",
       "      <td>14.672661</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.0</td>\n",
       "      <td>7.979585</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>10.490092</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>4.0</td>\n",
       "      <td>16.561155</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2.0</td>\n",
       "      <td>13.727386</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     n_cars  traffic_speed           date_time camera_id\n",
       "0      18.0      13.292431 2024-11-18 15:45:00         0\n",
       "1      19.0      10.643991 2024-11-18 15:45:00         0\n",
       "2      20.0      14.672661 2024-11-18 15:45:00         0\n",
       "3      17.0       7.979585 2024-11-18 15:45:00         0\n",
       "4      17.0      10.490092 2024-11-18 15:45:00         0\n",
       "..      ...            ...                 ...       ...\n",
       "126     9.0      -1.000000 2024-11-18 15:45:00         0\n",
       "127     3.0      -1.000000 2024-11-18 15:45:00         0\n",
       "128     4.0      16.561155 2024-11-18 15:45:00         0\n",
       "129     2.0      13.727386 2024-11-18 15:45:00         0\n",
       "130     2.0      -1.000000 2024-11-18 15:45:00         0\n",
       "\n",
       "[131 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEMPORARY until james updates the data\n",
    "# Data types\n",
    "# Camera ID, Date-Time (Y-M-D-H-M) Weekend WeekofMonth Season Speed (traffic_speed) Density (n_cars)\n",
    "concat_df['date_time'] = pd.to_datetime('2024-11-18 15:45:00')\n",
    "concat_df['camera_id'] = '0'\n",
    "concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "306236d5-f217-4356-9bad-222838598e3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extrapolate data\n",
    "concat_df[['year', 'month', 'day', 'hour', 'minute', 'is_weekend', 'week_of_month', 'season']] = concat_df['date_time'].apply(lambda x: pd.Series(extract_date_info(x)))\n",
    "concat_df\n",
    "\n",
    "# Save unnormalized data locally and in s3\n",
    "local_file_path = save_preprocessed_data_to_local(concat_df)  # Save locally first\n",
    "input_data = upload_to_s3(local_file_path, output_bucket_name, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1476c184-f277-4422-be88-820610f0ac4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_cars</th>\n",
       "      <th>traffic_speed</th>\n",
       "      <th>camera_id</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>minute_sin</th>\n",
       "      <th>minute_cos</th>\n",
       "      <th>week_of_month_sin</th>\n",
       "      <th>week_of_month_cos</th>\n",
       "      <th>season_sin</th>\n",
       "      <th>season_cos</th>\n",
       "      <th>year_sin</th>\n",
       "      <th>year_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.144153</td>\n",
       "      <td>0.243093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.485302</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-0.951057</td>\n",
       "      <td>0.309017</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0.587785</td>\n",
       "      <td>-0.809017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.333400</td>\n",
       "      <td>-0.054931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.485302</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-0.951057</td>\n",
       "      <td>0.309017</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0.587785</td>\n",
       "      <td>-0.809017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.522648</td>\n",
       "      <td>0.398408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.485302</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-0.951057</td>\n",
       "      <td>0.309017</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0.587785</td>\n",
       "      <td>-0.809017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.954905</td>\n",
       "      <td>-0.354753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.485302</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-0.951057</td>\n",
       "      <td>0.309017</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0.587785</td>\n",
       "      <td>-0.809017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.954905</td>\n",
       "      <td>-0.072249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.485302</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-0.951057</td>\n",
       "      <td>0.309017</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0.587785</td>\n",
       "      <td>-0.809017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     n_cars  traffic_speed  camera_id  is_weekend  month_sin  month_cos  \\\n",
       "0  1.144153       0.243093        0.0         0.0       -0.5   0.866025   \n",
       "1  1.333400      -0.054931        0.0         0.0       -0.5   0.866025   \n",
       "2  1.522648       0.398408        0.0         0.0       -0.5   0.866025   \n",
       "3  0.954905      -0.354753        0.0         0.0       -0.5   0.866025   \n",
       "4  0.954905      -0.072249        0.0         0.0       -0.5   0.866025   \n",
       "\n",
       "    day_sin   day_cos  hour_sin  hour_cos  minute_sin    minute_cos  \\\n",
       "0 -0.485302 -0.874347 -0.707107 -0.707107        -1.0 -1.836970e-16   \n",
       "1 -0.485302 -0.874347 -0.707107 -0.707107        -1.0 -1.836970e-16   \n",
       "2 -0.485302 -0.874347 -0.707107 -0.707107        -1.0 -1.836970e-16   \n",
       "3 -0.485302 -0.874347 -0.707107 -0.707107        -1.0 -1.836970e-16   \n",
       "4 -0.485302 -0.874347 -0.707107 -0.707107        -1.0 -1.836970e-16   \n",
       "\n",
       "   week_of_month_sin  week_of_month_cos  season_sin    season_cos  year_sin  \\\n",
       "0          -0.951057           0.309017        -1.0 -1.836970e-16  0.587785   \n",
       "1          -0.951057           0.309017        -1.0 -1.836970e-16  0.587785   \n",
       "2          -0.951057           0.309017        -1.0 -1.836970e-16  0.587785   \n",
       "3          -0.951057           0.309017        -1.0 -1.836970e-16  0.587785   \n",
       "4          -0.951057           0.309017        -1.0 -1.836970e-16  0.587785   \n",
       "\n",
       "   year_cos  \n",
       "0 -0.809017  \n",
       "1 -0.809017  \n",
       "2 -0.809017  \n",
       "3 -0.809017  \n",
       "4 -0.809017  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df = preprocess_expanded_df(concat_df)\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc914c31-37c4-410a-8cfd-ed8083c6830a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "test_inference = preprocessed_df.head(1)  # Get the first row\n",
    "# test_inference_csv = test_inference.to_csv(index=False, header=False).encode('utf-8')\n",
    "# Extract the first row\n",
    "\n",
    "# Convert to numpy array and encode as float32\n",
    "test_inference_float32 = test_inference.to_numpy().astype(np.float32)\n",
    "\n",
    "# Verify the dtype\n",
    "print(test_inference_float32.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caffb156-a564-4b60-9486-0db6045894e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name sagemaker_notebook_jupyter_lab to get Role path.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/fsspec/registry.py:279: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: randomcutforest-2024-11-27-01-24-32-292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-27 01:24:34 Starting - Starting the training job...\n",
      "2024-11-27 01:24:49 Starting - Preparing the instances for training...\n",
      "2024-11-27 01:25:16 Downloading - Downloading input data...\n",
      "2024-11-27 01:25:41 Downloading - Downloading the training image...............\n",
      "2024-11-27 01:28:38 Training - Training image download completed. Training in progress....\n",
      "2024-11-27 01:29:09 Uploading - Uploading generated training model...\n",
      "2024-11-27 01:29:22 Completed - Training job completed\n",
      "..Training seconds: 245\n",
      "Billable seconds: 245\n"
     ]
    }
   ],
   "source": [
    "# Train the model (input_data is a filepath)\n",
    "rcf_model = train_rcf_model(input_data, bucket_name=output_bucket_name, output_data_file_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e35d1243-0a82-49c3-9b2e-efa12fc211da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # TODO: Fix load model weights (save is handled by aws)\n",
    "# # Define S3 bucket and model location\n",
    "# bucket_name = session.default_bucket()\n",
    "# model_artifacts_s3_path = 's3://{}/rcf-model-output/model.tar.gz'  # The S3 path where your model is stored\n",
    "\n",
    "# # Load the trained model from S3\n",
    "# rcf_model = RandomCutForest.attach(model_data=model_artifacts_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3150c508-c222-47d1-800c-dac19e19f300",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating model with name: randomcutforest-2024-11-27-01-29-50-410\n",
      "INFO:sagemaker:Creating endpoint-config with name randomcutforest-2024-11-27-01-29-50-410\n",
      "INFO:sagemaker:Creating endpoint with name randomcutforest-2024-11-27-01-29-50-410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "def deploy_rcf_endpoint(rcf_model, instance_type='ml.m4.xlarge', initial_instance_count=1):\n",
    "    \"\"\"\n",
    "    Deploys the trained RCF model to a SageMaker endpoint.\n",
    "\n",
    "    Parameters:\n",
    "    - rcf_model: The trained RCF model.\n",
    "    - instance_type (str): The type of instance for the endpoint (default: 'ml.m4.xlarge').\n",
    "    - initial_instance_count (int): The number of instances for the endpoint (default: 1).\n",
    "\n",
    "    Returns:\n",
    "    - Predictor: The deployed model's predictor object.\n",
    "    \"\"\"\n",
    "    # Deploy the model\n",
    "    rcf_predictor = rcf_model.deploy(\n",
    "        instance_type=instance_type,\n",
    "        initial_instance_count=initial_instance_count,\n",
    "    )\n",
    "\n",
    "    # Configure the serializer and deserializer\n",
    "    rcf_predictor.serializer = CSVSerializer()  # Input will be CSV\n",
    "    rcf_predictor.deserializer = JSONDeserializer()  # Output will be JSON\n",
    "    \n",
    "    return rcf_predictor\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Deploy the endpoint\n",
    "rcf_detector = deploy_rcf_endpoint(rcf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a326d5cc-96b7-49e1-84c9-50112d0d43f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text/csv ('application/json',)\n"
     ]
    }
   ],
   "source": [
    "print(rcf_detector.content_type, rcf_detector.accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "741112ef-b76d-4e39-8e91-91d6f9eaa906",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload sent to endpoint:\n",
      "1.1441527605056763,0.24309329688549042,0.0,0.0,-0.5,0.8660253882408142,-0.4853019714355469,-0.8743466138839722,-0.7071067690849304,-0.7071067690849304,-1.0,-1.8369701465288538e-16,-0.9510565400123596,0.30901700258255005,-1.0,-1.8369701465288538e-16,0.5877852439880371,-0.80901700258255\n",
      "Error invoking the endpoint: 0\n",
      "Failed to get prediction\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "def invoke_rcf_endpoint(rcf_predictor, input_data):\n",
    "    \"\"\"\n",
    "    Sends data to the deployed RCF endpoint for inference.\n",
    "\n",
    "    Parameters:\n",
    "    - rcf_predictor: The deployed model's predictor object.\n",
    "    - input_data (pd.DataFrame or numpy.ndarray): Preprocessed input data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The JSON response from the endpoint.\n",
    "    \"\"\"\n",
    "    # Ensure input_data is in the correct format for prediction (float32)\n",
    "    if isinstance(input_data, pd.DataFrame):\n",
    "        input_data = input_data.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        input_data = input_data.to_numpy().astype('float32')\n",
    "    elif isinstance(input_data, np.ndarray):\n",
    "        input_data = input_data.astype('float32')\n",
    "\n",
    "    # Convert numpy array to CSV string (no header, no index)\n",
    "    input_data_csv = '\\n'.join([','.join(map(str, row)) for row in input_data.tolist()])\n",
    "\n",
    "    # Print the payload to inspect the format (optional)\n",
    "    print(\"Payload sent to endpoint:\")\n",
    "    print(input_data_csv)  # CSV format (without header or index)\n",
    "\n",
    "    try:\n",
    "        # Convert to JSON format expected by the endpoint (wrap CSV in a JSON structure)\n",
    "        payload = {\"data\": input_data_csv}\n",
    "\n",
    "        # Set content type to 'application/json' as required by the endpoint\n",
    "        response = rcf_predictor.predict(payload)\n",
    "\n",
    "        # Return the response (which is expected to be JSON)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking the endpoint: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'rcf_predictor' is your deployed RandomCutForest endpoint predictor\n",
    "\n",
    "example_data = test_inference_float32\n",
    "result = invoke_rcf_endpoint(rcf_detector, example_data)\n",
    "\n",
    "# Handle the result\n",
    "if result:\n",
    "    print(\"Prediction result:\", result)\n",
    "else:\n",
    "    print(\"Failed to get prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61910ac4-4cd5-4ecb-a270-9be6b083dac8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload sent to endpoint:\n",
      "1.1441527605056763,0.24309329688549042,0.0,0.0,-0.5,0.8660253882408142,-0.4853019714355469,-0.8743466138839722,-0.7071067690849304,-0.7071067690849304,-1.0,-1.8369701465288538e-16,-0.9510565400123596,0.30901700258255005,-1.0,-1.8369701465288538e-16,0.5877852439880371,-0.80901700258255\n",
      "Error invoking the endpoint: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"unable to evaluate payload provided\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/randomcutforest-2024-11-27-01-29-50-410 in account 559050226112 for more information.\n",
      "Failed to get prediction\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "def invoke_rcf_endpoint(rcf_predictor, input_data):\n",
    "    \"\"\"\n",
    "    Sends data to the deployed RCF endpoint for inference.\n",
    "\n",
    "    Parameters:\n",
    "    - rcf_predictor: The deployed model's predictor object.\n",
    "    - input_data (pd.DataFrame or numpy.ndarray): Preprocessed input data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The JSON response from the endpoint.\n",
    "    \"\"\"\n",
    "    # Ensure input_data is in the correct format for prediction (float32)\n",
    "    if isinstance(input_data, pd.DataFrame):\n",
    "        input_data = input_data.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        input_data = input_data.to_numpy().astype('float32')\n",
    "    elif isinstance(input_data, np.ndarray):\n",
    "        input_data = input_data.astype('float32')\n",
    "\n",
    "    # Convert numpy array to CSV string (no header, no index)\n",
    "    input_data_csv = '\\n'.join([','.join(map(str, row)) for row in input_data.tolist()])\n",
    "\n",
    "    # Print the payload to inspect the format (optional)\n",
    "    print(\"Payload sent to endpoint:\")\n",
    "    print(input_data_csv)  # CSV format (without header or index)\n",
    "\n",
    "    try:\n",
    "        # Invoke the endpoint for inference\n",
    "        response = rcf_predictor.predict(input_data_csv)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking the endpoint: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'rcf_predictor' is your deployed RandomCutForest endpoint predictor\n",
    "\n",
    "example_data = test_inference\n",
    "\n",
    "result = invoke_rcf_endpoint(rcf_detector, example_data)\n",
    "\n",
    "# Handle the result\n",
    "if result:\n",
    "    print(\"Prediction result:\", result)\n",
    "else:\n",
    "    print(\"Failed to get prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fedcc1ca-f365-4152-9070-3b74c25caa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: randomcutforest-2024-11-26-23-44-40-202\n",
      "INFO:sagemaker:Deleting endpoint with name: randomcutforest-2024-11-26-23-44-40-202\n"
     ]
    }
   ],
   "source": [
    "# Delete the endpoint (clean up)\n",
    "rcf_detector.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e77a4764-9b9e-4517-bf1a-f5708428a84f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating model with name: randomcutforest-2024-11-22-08-45-23-068\n",
      "INFO:sagemaker:Creating endpoint-config with name randomcutforest-2024-11-22-08-45-23-068\n",
      "INFO:sagemaker:Creating endpoint with name randomcutforest-2024-11-22-08-45-23-068\n",
      "WARNING:sagemaker:Received AccessDeniedException. This could mean the IAM role does not have the resource permissions, in which case please add resource access and retry. For cases where the role has tag based resource policy, continuing to wait for tag propagation..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Deploy the model for inference\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m rcf_inference \u001b[38;5;241m=\u001b[39m \u001b[43mrcf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mml.m4.xlarge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# # Set the serializer and deserializer for the endpoint\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# rcf_inference.serializer = CSVSerializer()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# rcf_inference.deserializer = JSONDeserializer()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Clean up the endpoint after inference\u001b[39;00m\n\u001b[1;32m     41\u001b[0m rcf_inference\u001b[38;5;241m.\u001b[39mdelete_endpoint()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/estimator.py:1712\u001b[0m, in \u001b[0;36mEstimatorBase.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, serverless_inference_config, async_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m   1706\u001b[0m model\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m model_name\n\u001b[1;32m   1708\u001b[0m tags \u001b[38;5;241m=\u001b[39m update_inference_tags_with_jumpstart_training_tags(\n\u001b[1;32m   1709\u001b[0m     inference_tags\u001b[38;5;241m=\u001b[39mformat_tags(tags), training_tags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[1;32m   1710\u001b[0m )\n\u001b[0;32m-> 1712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_instance_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1717\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_capture_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_capture_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserverless_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserverless_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvolume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvolume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_data_download_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_data_download_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontainer_startup_health_check_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontainer_startup_health_check_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference_recommendation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_recommendation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/model.py:1753\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, accept_eula, endpoint_logging, resources, endpoint_type, managed_instance_scaling, inference_component_name, routing_config, model_reference_arn, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_explainer_enabled:\n\u001b[1;32m   1751\u001b[0m     explainer_config_dict \u001b[38;5;241m=\u001b[39m explainer_config\u001b[38;5;241m.\u001b[39m_to_request_dict()\n\u001b[0;32m-> 1753\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_from_production_variants\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproduction_variants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mproduction_variant\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls:\n\u001b[1;32m   1766\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:5820\u001b[0m, in \u001b[0;36mSession.endpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict, live_logging, vpc_config, enable_network_isolation, role)\u001b[0m\n\u001b[1;32m   5817\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating endpoint-config with name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[1;32m   5818\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_endpoint_config(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_options)\n\u001b[0;32m-> 5820\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlive_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5826\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:4654\u001b[0m, in \u001b[0;36mSession.create_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait, live_logging)\u001b[0m\n\u001b[1;32m   4651\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_arn \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEndpointArn\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   4653\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 4654\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlive_logging\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m endpoint_name\n\u001b[1;32m   4656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:5434\u001b[0m, in \u001b[0;36mSession.wait_for_endpoint\u001b[0;34m(self, endpoint, poll, live_logging)\u001b[0m\n\u001b[1;32m   5419\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for an Amazon SageMaker endpoint deployment to complete.\u001b[39;00m\n\u001b[1;32m   5420\u001b[0m \n\u001b[1;32m   5421\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5430\u001b[0m \u001b[38;5;124;03m    dict: Return value from the ``DescribeEndpoint`` API.\u001b[39;00m\n\u001b[1;32m   5431\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m live_logging \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _has_permission_for_live_logging(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboto_session, endpoint):\n\u001b[0;32m-> 5434\u001b[0m     desc \u001b[38;5;241m=\u001b[39m \u001b[43m_wait_until\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_deploy_done\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5436\u001b[0m     cloudwatch_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboto_session\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:8358\u001b[0m, in \u001b[0;36m_wait_until\u001b[0;34m(callable_fn, poll)\u001b[0m\n\u001b[1;32m   8356\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   8357\u001b[0m     elapsed_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m poll\n\u001b[0;32m-> 8358\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   8359\u001b[0m     result \u001b[38;5;241m=\u001b[39m callable_fn()\n\u001b[1;32m   8360\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m botocore\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mClientError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   8361\u001b[0m     \u001b[38;5;66;03m# For initial 5 mins we accept/pass AccessDeniedException.\u001b[39;00m\n\u001b[1;32m   8362\u001b[0m     \u001b[38;5;66;03m# The reason is to await tag propagation to avoid false AccessDenied claims for an\u001b[39;00m\n\u001b[1;32m   8363\u001b[0m     \u001b[38;5;66;03m# access policy based on resource tags, The caveat here is for true AccessDenied\u001b[39;00m\n\u001b[1;32m   8364\u001b[0m     \u001b[38;5;66;03m# cases the routine will fail after 5 mins\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from sagemaker.serializers import CSVSerializer\n",
    "# from sagemaker.deserializers import JSONDeserializer\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Deploy the model for inference\n",
    "# rcf_inference = rcf_model.deploy(\n",
    "#     initial_instance_count=1,\n",
    "#     instance_type='ml.m4.xlarge',\n",
    "# )\n",
    "\n",
    "# # # Set the serializer and deserializer for the endpoint\n",
    "# # rcf_inference.serializer = CSVSerializer()\n",
    "# # rcf_inference.deserializer = JSONDeserializer()\n",
    "\n",
    "# # # Prepare inference data from the preprocessed DataFrame (already normalized)\n",
    "# # # For this example, let's use the first row (you can select another row or the whole dataset)\n",
    "# # inference_data = preprocessed_df.iloc[0, :].values.reshape(1, -1)  # Use the first row for inference\n",
    "\n",
    "# # # Make predictions by invoking the endpoint\n",
    "# # results = rcf_inference.predict(inference_data)\n",
    "\n",
    "# # # Convert the results to a DataFrame for easier analysis\n",
    "# # results_pd = pd.DataFrame(results['scores'])\n",
    "\n",
    "# # # Define threshold using 3-sigma rule (mean + 3 * std)\n",
    "# # threshold = results_pd.mean() + results_pd.std() * 3\n",
    "# # print('3 sigma threshold : {}'.format(threshold.values[0]))\n",
    "\n",
    "# # # Plot the anomaly scores\n",
    "# # results_pd.hist()\n",
    "# # plt.title('Anomaly Scores')\n",
    "# # plt.axvline(x=2.62, linestyle=':', color='r')\n",
    "# # plt.annotate('Threshold=2.62', xy=(2.62, 300), color='r')\n",
    "# # plt.axvline(x=threshold.values, linestyle=':', color='k')\n",
    "# # plt.annotate(f'Threshold={round(threshold.values[0], 2)}', xy=(threshold.values, 150))\n",
    "# # plt.show()\n",
    "\n",
    "# # Clean up the endpoint after inference\n",
    "# rcf_inference.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4e8c8-6a12-4af7-a59f-f551f1109b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: output into csv file\n",
    "# data passed in, result\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9daa09-e57e-4d6c-9724-7b282a3db3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
