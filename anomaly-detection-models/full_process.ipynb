{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f2324f5-f71c-4363-a67a-1c3fd7d31a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "# pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6a404b-14a6-4a3c-88d6-bce5a6f195b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.15\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40844dcc-79c4-454d-80e2-5c8530b8d492",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, read_csv, concat\n",
    "import boto3\n",
    "import os\n",
    "from os import remove\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e29de-ddd1-48fc-bd7a-8c83001d9830",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c42e61a-b26b-4108-bf8f-f333330d5a5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pandas import DataFrame, read_csv, concat\n",
    "# import boto3\n",
    "# from os import remove\n",
    "\n",
    "def download_and_concat_s3_csvs(output_bucket_name, concatenated_csv):\n",
    "    \"\"\"\n",
    "    Downloads CSV files from the specified S3 bucket, concatenates them into a single DataFrame, and returns it.\n",
    "    \n",
    "    Parameters:\n",
    "    - output_bucket_name (str): The name of the S3 bucket containing the CSV files.\n",
    "    - concatenated_csv (str): The name of the concatenated output file to be saved locally.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The concatenated DataFrame.\n",
    "    \"\"\"\n",
    "    # Initialize S3 client\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # List files already in the output directory\n",
    "    written_output_files = s3.list_objects(Bucket=output_bucket_name, Prefix='csv/', Delimiter='/')\n",
    "    written_output_files = [i['Key'] for i in written_output_files.get('Contents', [])]\n",
    "\n",
    "    # Filter out empty keys or directories\n",
    "    csv_files = [key for key in written_output_files if key.endswith('.csv')]\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Process each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        local_filename = csv_file.split('/')[-1]  # Extract the file name\n",
    "        # Download the file locally\n",
    "        s3.download_file(output_bucket_name, csv_file, local_filename)\n",
    "        \n",
    "        # Load the CSV into a DataFrame\n",
    "        df = pd.read_csv(local_filename)\n",
    "        dataframes.append(df)\n",
    "        \n",
    "        # Clean up the local file\n",
    "        remove(local_filename)\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    concat_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Save the concatenated DataFrame locally if needed\n",
    "    concat_df.to_csv(concatenated_csv, index=False)\n",
    "\n",
    "    return concat_df\n",
    "\n",
    "# TODO: deal with this later\n",
    "# # Save the concatenated DataFrame to a new CSV file\n",
    "# final_df.to_csv(concatenated_csv, index=False)\n",
    "\n",
    "# # Optional: Upload the concatenated CSV back to S3\n",
    "# s3.upload_file(concatenated_csv, output_bucket_name, f'csv/{concatenated_csv}')\n",
    "\n",
    "# # Remove the local concatenated file\n",
    "# remove(concatenated_csv)\n",
    "\n",
    "# print(f\"Concatenated CSV saved to {concatenated_csv} and uploaded to bucket.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99083334-ae17-4fc9-aa7b-18b20854762e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# Take the datetime object and turn it into the other needed columns\n",
    "def extract_date_info(dt):\n",
    "    # Extract basic components\n",
    "    year = dt.year\n",
    "    month = dt.month\n",
    "    day = dt.day\n",
    "    hour = dt.hour\n",
    "    minute = dt.minute\n",
    "    weekday = dt.weekday()  # 0=Monday, 6=Sunday\n",
    "    is_weekend = weekday >= 5\n",
    "\n",
    "    # Calculate the week of the month\n",
    "    first_day_of_month = dt.replace(day=1)\n",
    "    week_of_month = (dt.day + first_day_of_month.weekday()) // 7 + 1\n",
    "\n",
    "    # Determine the season (Northern Hemisphere)\n",
    "    if month in (12, 1, 2):\n",
    "        season = \"Winter\"\n",
    "    elif month in (3, 4, 5):\n",
    "        season = \"Spring\"\n",
    "    elif month in (6, 7, 8):\n",
    "        season = \"Summer\"\n",
    "    else:\n",
    "        season = \"Fall\"\n",
    "\n",
    "    return year, month, day, hour, minute, is_weekend, week_of_month, season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f034d358-66e3-450d-a972-f9f9d2f386c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_expanded_df(data):\n",
    "    \"\"\"\n",
    "    Preprocess the expanded DataFrame for use with the AWS Random Cut Forest (RCF) model.\n",
    "    \n",
    "    This function:\n",
    "    - Removes `date_time`.\n",
    "    - Encodes and transforms categorical and cyclical features.\n",
    "    - Normalizes numerical features.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): Expanded DataFrame with columns:\n",
    "      ['n_cars', 'traffic_speed', 'date_time', 'camera_id', \n",
    "       'year', 'month', 'day', 'hour', 'minute', \n",
    "       'is_weekend', 'week_of_month', 'season'].\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed DataFrame ready for RCF.\n",
    "    \"\"\"\n",
    "    # Drop the `date_time` column\n",
    "    data = data.drop(columns=[\"date_time\"])\n",
    "    \n",
    "    # Encode `is_weekend` as integer (0 or 1)\n",
    "    data[\"is_weekend\"] = data[\"is_weekend\"].astype(int)\n",
    "    \n",
    "    # Map `season` to numeric values if it's of type object\n",
    "    if data[\"season\"].dtype == \"object\":\n",
    "        season_mapping = {\"Winter\": 0, \"Spring\": 1, \"Summer\": 2, \"Fall\": 3}\n",
    "        data[\"season\"] = data[\"season\"].map(season_mapping)\n",
    "        if data[\"season\"].isna().any():\n",
    "            raise ValueError(\"Unmapped season value detected in the data.\")\n",
    "    \n",
    "    # Define a function for cyclical transformations\n",
    "    def add_cyclical_features(data, column, max_value):\n",
    "        data[f\"{column}_sin\"] = np.sin(2 * np.pi * data[column] / max_value)\n",
    "        data[f\"{column}_cos\"] = np.cos(2 * np.pi * data[column] / max_value)\n",
    "        return data\n",
    "    \n",
    "    # Apply cyclical transformations\n",
    "    data = add_cyclical_features(data, \"month\", 12)          # Cyclical for months\n",
    "    data = add_cyclical_features(data, \"day\", 31)            # Cyclical for days\n",
    "    data = add_cyclical_features(data, \"hour\", 24)           # Cyclical for hours\n",
    "    data = add_cyclical_features(data, \"minute\", 60)         # Cyclical for minutes\n",
    "    data = add_cyclical_features(data, \"week_of_month\", 5)   # Cyclical for weeks of the month\n",
    "    data = add_cyclical_features(data, \"season\", 4)          # Cyclical for seasons\n",
    "    data = add_cyclical_features(data, \"year\", 10)           # Cyclical for years (assuming a 10-year cycle, adjust as needed)\n",
    "    \n",
    "    # Drop the original cyclical columns\n",
    "    columns_to_drop = [\"month\", \"day\", \"hour\", \"minute\", \"week_of_month\", \"season\", \"year\"]\n",
    "    data = data.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Assuming df is your DataFrame\n",
    "    data.loc[data['traffic_speed'] == -1, 'traffic_speed'] = 0\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    numerical_columns = [\n",
    "        \"n_cars\", \"traffic_speed\", \"camera_id\", \"is_weekend\"\n",
    "    ]\n",
    "    scaler = StandardScaler()\n",
    "    data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9355432b-c911-45ba-be21-dea4f9214b63",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33881baf-aa9e-43c2-b6ac-c3e54e22b0bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_preprocessed_data_to_local(data, filename='data.csv'):\n",
    "    \"\"\"\n",
    "    Saves the preprocessed data to a local CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): Preprocessed traffic data.\n",
    "    - filename (str): The name of the local file to be saved (default is 'data.csv').\n",
    "    \"\"\"\n",
    "    # Save the preprocessed data to CSV\n",
    "    data.to_csv(filename, header=False, index=False)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "406b5399-1687-4594-b413-07f012129fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_to_s3(local_file_path, bucket_name, s3_key):\n",
    "    \"\"\"\n",
    "    Uploads a local file to S3.\n",
    "\n",
    "    Parameters:\n",
    "    - local_file_path (str): The local file path of the CSV to be uploaded.\n",
    "    - bucket_name (str): The name of the S3 bucket.\n",
    "    - s3_key (str): The S3 object key (path where the file will be stored).\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_client.upload_file(local_file_path, bucket_name, s3_key)\n",
    "    \n",
    "    # Return the full S3 URI\n",
    "    return f\"s3://{bucket_name}/{s3_key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c643b1da-4d58-4559-b267-9934b81e67f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import RandomCutForest\n",
    "import pandas as pd\n",
    "\n",
    "def train_rcf_model(input_data, bucket_name, output_data_file_path, region_name='us-east-1'):\n",
    "    \"\"\"\n",
    "    Trains the RCF model using the preprocessed data stored locally or in S3.\n",
    "\n",
    "    Parameters:\n",
    "    - input_data (str): Local file path or S3 URI of the preprocessed data.\n",
    "    - bucket_name (str): The name of the S3 bucket for output data.\n",
    "    - output_data_file_path (str): The path in the S3 bucket where model output will be saved.\n",
    "    - region_name (str): The AWS region for the SageMaker model.\n",
    "\n",
    "    Returns:\n",
    "    - sagemaker.estimator.Estimator: The trained RCF model.\n",
    "    \"\"\"\n",
    "    # Get the SageMaker session and execution role\n",
    "    session = sagemaker.Session()\n",
    "    role = get_execution_role()\n",
    "\n",
    "    # Upload the input data to S3 if it's a local file\n",
    "    if input_data.startswith('s3://'):\n",
    "        input_data_location = input_data  # Data is already in S3\n",
    "    else:\n",
    "        input_data_location = session.upload_data(path=input_data, bucket=bucket_name, key_prefix='rcf-clickstream')\n",
    "\n",
    "    # Initialize the RandomCutForest estimator\n",
    "    rcf = RandomCutForest(\n",
    "        role=role,\n",
    "        train_instance_count=1,\n",
    "        train_instance_type='ml.m4.xlarge',\n",
    "        data_location=input_data_location,\n",
    "        output_path=f's3://{bucket_name}/{output_data_file_path}',  # Output location\n",
    "        num_samples_per_tree=256,  # Hyperparameter\n",
    "        num_trees=100,  # Hyperparameter\n",
    "        sagemaker_session=session\n",
    "    )\n",
    "\n",
    "    # Load training data (assuming it's in CSV format)\n",
    "    train_data = pd.read_csv(input_data)  # Assuming input is a CSV file\n",
    "    \n",
    "    # Ensure all data is numeric (e.g., convert to float32)\n",
    "    train_data = train_data.apply(pd.to_numeric, errors='coerce')  # Convert all columns to numeric, non-convertible become NaN\n",
    "    train_data = train_data.fillna(0)  # Replace NaN values with 0, if needed (or handle accordingly)\n",
    "    \n",
    "    # Convert to numpy array with float32 dtype\n",
    "    train_data_numpy = train_data.to_numpy().astype('float32')\n",
    "\n",
    "    # Convert the data to the record set format\n",
    "    rcf_record_set = rcf.record_set(train_data_numpy, channel='train', encrypt=False)\n",
    "\n",
    "    # Train the model\n",
    "    rcf.fit(rcf_record_set)\n",
    "\n",
    "    return rcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9528573e-125e-442f-8694-9850142a50d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# import sagemaker\n",
    "# from sagemaker import get_execution_role\n",
    "# from sagemaker.predictor import Predictor\n",
    "# from sagemaker import RandomCutForest\n",
    "# from sagemaker import Session\n",
    "\n",
    "# # WARNING Duplicate\n",
    "# # Deploy and predict\n",
    "# def deploy_and_predict(rcf_model, inference_data):\n",
    "#     \"\"\"\n",
    "#     Deploys the RCF model and makes a prediction.\n",
    "\n",
    "#     Parameters:\n",
    "#     - rcf_model (sagemaker.estimator.Estimator): The trained RCF model.\n",
    "#     - inference_data (np.array): The data to be used for inference.\n",
    "\n",
    "#     Returns:\n",
    "#     - dict: The predicted anomaly scores.\n",
    "#     \"\"\"\n",
    "#     # Deploy the model\n",
    "#     rcf_predictor = rcf_model.deploy(\n",
    "#         initial_instance_count=1,\n",
    "#         instance_type='ml.t3.medium'\n",
    "#     )\n",
    "    \n",
    "#     # Prepare the payload for inference (convert to numpy array and list)\n",
    "#     payload = json.dumps(inference_data.tolist())\n",
    "\n",
    "#     # Make a prediction (send payload to the deployed model)\n",
    "#     response = rcf_predictor.predict(payload)\n",
    "    \n",
    "#     # The response needs to be decoded and parsed from the result\n",
    "#     result = json.loads(response)\n",
    "\n",
    "#     print(\"Anomaly scores:\", result)\n",
    "\n",
    "#     # Clean up the endpoint after inference\n",
    "#     rcf_predictor.delete_endpoint()\n",
    "\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b234ab-4abe-4292-91c7-bf9ce65c7dd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a15d5e1b-107b-487a-a549-1575e4df3f64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   n_cars  traffic_speed\n",
      "0    29.0      14.734943\n",
      "1    27.0      11.417725\n",
      "2    40.0      14.436572\n",
      "3    23.0      10.827753\n",
      "4    30.0      12.084542\n"
     ]
    }
   ],
   "source": [
    "# Usage of the function\n",
    "input_bucket_name = 'jfrechmsml650output'\n",
    "concatenated_csv = 'concatenated_output.csv'\n",
    "output_bucket_name = 'dulcichmsml650bucket' # Make and enter bucket name\n",
    "output_path = 'output.csv' # ex. data/traffic_data.csv\n",
    "local_file_path = ''\n",
    "\n",
    "# Get the concatenated DataFrame\n",
    "concat_df = download_and_concat_s3_csvs(input_bucket_name, concatenated_csv)\n",
    "\n",
    "# Optionally, print the first few rows of the concatenated DataFrame\n",
    "print(concat_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "789e9c4c-a1ef-4dc4-9b42-67aff9c6dbe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_cars</th>\n",
       "      <th>traffic_speed</th>\n",
       "      <th>date_time</th>\n",
       "      <th>camera_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.0</td>\n",
       "      <td>14.734943</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>11.417725</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.0</td>\n",
       "      <td>14.436572</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.0</td>\n",
       "      <td>10.827753</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.0</td>\n",
       "      <td>12.084542</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.959250</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>37.0</td>\n",
       "      <td>2.663268</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>15.0</td>\n",
       "      <td>3.130074</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>25.0</td>\n",
       "      <td>3.004026</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     n_cars  traffic_speed           date_time camera_id\n",
       "0      29.0      14.734943 2024-11-18 15:45:00         0\n",
       "1      27.0      11.417725 2024-11-18 15:45:00         0\n",
       "2      40.0      14.436572 2024-11-18 15:45:00         0\n",
       "3      23.0      10.827753 2024-11-18 15:45:00         0\n",
       "4      30.0      12.084542 2024-11-18 15:45:00         0\n",
       "..      ...            ...                 ...       ...\n",
       "249     9.0      -1.000000 2024-11-18 15:45:00         0\n",
       "250    17.0       2.959250 2024-11-18 15:45:00         0\n",
       "251    37.0       2.663268 2024-11-18 15:45:00         0\n",
       "252    15.0       3.130074 2024-11-18 15:45:00         0\n",
       "253    25.0       3.004026 2024-11-18 15:45:00         0\n",
       "\n",
       "[254 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEMPORARY until james updates the data\n",
    "# Data types\n",
    "# Camera ID, Date-Time (Y-M-D-H-M) Weekend WeekofMonth Season Speed (traffic_speed) Density (n_cars)\n",
    "# concat_df['date_time'] = pd.to_datetime('2024-11-18 15:45:00')\n",
    "# concat_df['camera_id'] = '0'\n",
    "# concat_df\n",
    "\n",
    "# Final to fix date-time\n",
    "concat_df['date_time'] = pd.to_datetime(concat_df['date_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "306236d5-f217-4356-9bad-222838598e3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_cars</th>\n",
       "      <th>traffic_speed</th>\n",
       "      <th>date_time</th>\n",
       "      <th>camera_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.0</td>\n",
       "      <td>14.734943</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>11.417725</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.0</td>\n",
       "      <td>14.436572</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.0</td>\n",
       "      <td>10.827753</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.0</td>\n",
       "      <td>12.084542</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.959250</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>37.0</td>\n",
       "      <td>2.663268</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>15.0</td>\n",
       "      <td>3.130074</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>25.0</td>\n",
       "      <td>3.004026</td>\n",
       "      <td>2024-11-18 15:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     n_cars  traffic_speed           date_time camera_id  year  month  day  \\\n",
       "0      29.0      14.734943 2024-11-18 15:45:00         0  2024     11   18   \n",
       "1      27.0      11.417725 2024-11-18 15:45:00         0  2024     11   18   \n",
       "2      40.0      14.436572 2024-11-18 15:45:00         0  2024     11   18   \n",
       "3      23.0      10.827753 2024-11-18 15:45:00         0  2024     11   18   \n",
       "4      30.0      12.084542 2024-11-18 15:45:00         0  2024     11   18   \n",
       "..      ...            ...                 ...       ...   ...    ...  ...   \n",
       "249     9.0      -1.000000 2024-11-18 15:45:00         0  2024     11   18   \n",
       "250    17.0       2.959250 2024-11-18 15:45:00         0  2024     11   18   \n",
       "251    37.0       2.663268 2024-11-18 15:45:00         0  2024     11   18   \n",
       "252    15.0       3.130074 2024-11-18 15:45:00         0  2024     11   18   \n",
       "253    25.0       3.004026 2024-11-18 15:45:00         0  2024     11   18   \n",
       "\n",
       "     hour  minute  is_weekend  week_of_month season  \n",
       "0      15      45       False              4   Fall  \n",
       "1      15      45       False              4   Fall  \n",
       "2      15      45       False              4   Fall  \n",
       "3      15      45       False              4   Fall  \n",
       "4      15      45       False              4   Fall  \n",
       "..    ...     ...         ...            ...    ...  \n",
       "249    15      45       False              4   Fall  \n",
       "250    15      45       False              4   Fall  \n",
       "251    15      45       False              4   Fall  \n",
       "252    15      45       False              4   Fall  \n",
       "253    15      45       False              4   Fall  \n",
       "\n",
       "[254 rows x 12 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extrapolate data\n",
    "concat_df[['year', 'month', 'day', 'hour', 'minute', 'is_weekend', 'week_of_month', 'season']] = concat_df['date_time'].apply(lambda x: pd.Series(extract_date_info(x)))\n",
    "concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1476c184-f277-4422-be88-820610f0ac4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_df = preprocess_expanded_df(concat_df)\n",
    "preprocessed_df.head()\n",
    "\n",
    "# Save unnormalized data locally and in s3\n",
    "local_file_path = save_preprocessed_data_to_local(preprocessed_df)  # Save locally first\n",
    "input_data = upload_to_s3(local_file_path, output_bucket_name, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "073f7f3a-4df4-432d-8c45-c26dc5815c91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "# Test inference data\n",
    "test_inference = preprocessed_df.head() # Get the first row\n",
    "# test_inference_csv = test_inference.to_csv(index=False, header=False).encode('utf-8')\n",
    "# Extract the first row\n",
    "\n",
    "# Convert to numpy array and encode as float32\n",
    "# test_inference_float32 = test_inference.to_numpy().astype(np.float32)\n",
    "\n",
    "# # Verify the dtype\n",
    "print(test_inference.to_numpy().dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caffb156-a564-4b60-9486-0db6045894e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name sagemaker_notebook_jupyter_lab to get Role path.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/fsspec/registry.py:279: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: randomcutforest-2024-12-02-16-50-28-808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-02 16:50:29 Starting - Starting the training job...\n",
      "2024-12-02 16:50:50 Starting - Preparing the instances for training...\n",
      "2024-12-02 16:51:22 Downloading - Downloading input data...\n",
      "2024-12-02 16:51:52 Downloading - Downloading the training image......\n",
      "2024-12-02 16:53:03 Training - Training image download completed. Training in progress.......\n",
      "2024-12-02 16:53:46 Uploading - Uploading generated training model\n",
      "2024-12-02 16:53:46 Completed - Training job completed\n",
      "..Training seconds: 145\n",
      "Billable seconds: 145\n"
     ]
    }
   ],
   "source": [
    "# Train the model (input_data is a filepath)\n",
    "rcf_model = train_rcf_model(input_data, bucket_name=output_bucket_name, output_data_file_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e35d1243-0a82-49c3-9b2e-efa12fc211da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # TODO: Fix load model weights (save is handled by aws)\n",
    "# # Define S3 bucket and model location\n",
    "# bucket_name = session.default_bucket()\n",
    "# model_artifacts_s3_path = 's3://{}/rcf-model-output/model.tar.gz'  # The S3 path where your model is stored\n",
    "\n",
    "# # Load the trained model from S3\n",
    "# rcf_model = RandomCutForest.attach(model_data=model_artifacts_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3150c508-c222-47d1-800c-dac19e19f300",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating model with name: randomcutforest-2024-12-02-16-54-15-915\n",
      "INFO:sagemaker:Creating endpoint-config with name randomcutforest-2024-12-02-16-54-15-915\n",
      "INFO:sagemaker:Creating endpoint with name randomcutforest-2024-12-02-16-54-15-915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "def deploy_rcf_endpoint(rcf_model, instance_type='ml.m4.xlarge', initial_instance_count=1):\n",
    "    \"\"\"\n",
    "    Deploys the trained RCF model to a SageMaker endpoint.\n",
    "\n",
    "    Parameters:\n",
    "    - rcf_model: The trained RCF model.\n",
    "    - instance_type (str): The type of instance for the endpoint (default: 'ml.m4.xlarge').\n",
    "    - initial_instance_count (int): The number of instances for the endpoint (default: 1).\n",
    "\n",
    "    Returns:\n",
    "    - Predictor: The deployed model's predictor object.\n",
    "    \"\"\"\n",
    "    # Deploy the model\n",
    "    rcf_predictor = rcf_model.deploy(\n",
    "        instance_type=instance_type,\n",
    "        initial_instance_count=initial_instance_count,\n",
    "    )\n",
    "\n",
    "    # Configure the serializer and deserializer\n",
    "    rcf_predictor.serializer = CSVSerializer()  # Input will be CSV\n",
    "    rcf_predictor.deserializer = JSONDeserializer()  # Output will be JSON\n",
    "    \n",
    "    return rcf_predictor\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Deploy the endpoint\n",
    "rcf_detector = deploy_rcf_endpoint(rcf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a326d5cc-96b7-49e1-84c9-50112d0d43f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text/csv ('application/json',)\n"
     ]
    }
   ],
   "source": [
    "print(rcf_detector.content_type, rcf_detector.accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61910ac4-4cd5-4ecb-a270-9be6b083dac8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload sent to endpoint:\n",
      "1.5181864500045776,0.11736033111810684,0.0,0.0,-0.5,0.8660253882408142,-0.4853019714355469,-0.8743466138839722,-0.7071067690849304,-0.7071067690849304,-1.0,-1.8369701465288538e-16,-0.9510565400123596,0.30901700258255005,-1.0,-1.8369701465288538e-16,0.5877852439880371,-0.80901700258255\n",
      "1.2840520143508911,-0.17972011864185333,0.0,0.0,-0.5,0.8660253882408142,-0.4853019714355469,-0.8743466138839722,-0.7071067690849304,-0.7071067690849304,-1.0,-1.8369701465288538e-16,-0.9510565400123596,0.30901700258255005,-1.0,-1.8369701465288538e-16,0.5877852439880371,-0.80901700258255\n",
      "2.8059256076812744,0.09063906967639923,0.0,0.0,-0.5,0.8660253882408142,-0.4853019714355469,-0.8743466138839722,-0.7071067690849304,-0.7071067690849304,-1.0,-1.8369701465288538e-16,-0.9510565400123596,0.30901700258255005,-1.0,-1.8369701465288538e-16,0.5877852439880371,-0.80901700258255\n",
      "0.8157832622528076,-0.23255623877048492,0.0,0.0,-0.5,0.8660253882408142,-0.4853019714355469,-0.8743466138839722,-0.7071067690849304,-0.7071067690849304,-1.0,-1.8369701465288538e-16,-0.9510565400123596,0.30901700258255005,-1.0,-1.8369701465288538e-16,0.5877852439880371,-0.80901700258255\n",
      "1.635253667831421,-0.12000186741352081,0.0,0.0,-0.5,0.8660253882408142,-0.4853019714355469,-0.8743466138839722,-0.7071067690849304,-0.7071067690849304,-1.0,-1.8369701465288538e-16,-0.9510565400123596,0.30901700258255005,-1.0,-1.8369701465288538e-16,0.5877852439880371,-0.80901700258255\n",
      "Prediction result: {'scores': [{'score': 0.7528421222}, {'score': 0.73545973}, {'score': 0.8202757705}, {'score': 0.7006394211}, {'score': 0.7585047419}]}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "def invoke_rcf_endpoint(rcf_predictor, input_data):\n",
    "    \"\"\"\n",
    "    Sends data to the deployed RCF endpoint for inference.\n",
    "\n",
    "    Parameters:\n",
    "    - rcf_predictor: The deployed model's predictor object.\n",
    "    - input_data (pd.DataFrame or numpy.ndarray): Preprocessed input data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The JSON response from the endpoint.\n",
    "    \"\"\"\n",
    "    # Ensure input_data is in the correct format for prediction (float32)\n",
    "    if isinstance(input_data, pd.DataFrame):\n",
    "        input_data = input_data.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        input_data = input_data.to_numpy().astype('float32')\n",
    "    elif isinstance(input_data, np.ndarray):\n",
    "        input_data = input_data.astype('float32')\n",
    "\n",
    "    # Convert numpy array to CSV string (no header, no index)\n",
    "    input_data_csv = '\\n'.join([','.join(map(str, row)) for row in input_data.tolist()])\n",
    "\n",
    "    # Print the payload to inspect the format (optional)\n",
    "    print(\"Payload sent to endpoint:\")\n",
    "    print(input_data_csv)  # CSV format (without header or index)\n",
    "\n",
    "    try:\n",
    "        # Invoke the endpoint for inference\n",
    "        response = rcf_predictor.predict(input_data_csv)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking the endpoint: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'rcf_predictor' is your deployed RandomCutForest endpoint predictor\n",
    "\n",
    "example_data = test_inference\n",
    "\n",
    "result = invoke_rcf_endpoint(rcf_detector, example_data)\n",
    "\n",
    "# Handle the result\n",
    "if result:\n",
    "    print(\"Prediction result:\", result)\n",
    "else:\n",
    "    print(\"Failed to get prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b32dde05-c992-4ce8-9560-1d0d237a4aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly scores: [0.7528421222, 0.73545973, 0.8202757705, 0.7006394211, 0.7585047419]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "def invoke_rcf_endpoint(endpoint_name, input_data):\n",
    "    \"\"\"\n",
    "    Sends data to a deployed Random Cut Forest endpoint for inference.\n",
    "\n",
    "    Parameters:\n",
    "    - endpoint_name (str): The name of the SageMaker endpoint.\n",
    "    - input_data (pd.DataFrame or np.ndarray): Preprocessed input data.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of anomaly scores.\n",
    "    \"\"\"\n",
    "    # Ensure input_data is in the correct format\n",
    "    if isinstance(input_data, pd.DataFrame):\n",
    "        input_data = input_data.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        input_data = input_data.to_numpy().astype('float32')\n",
    "    elif isinstance(input_data, np.ndarray):\n",
    "        input_data = input_data.astype('float32')\n",
    "    else:\n",
    "        raise ValueError(\"Input data must be a pandas DataFrame or numpy ndarray.\")\n",
    "\n",
    "    # Convert numpy array to CSV format\n",
    "    input_data_csv = '\\n'.join([','.join(map(str, row)) for row in input_data.tolist()])\n",
    "\n",
    "    # Initialize SageMaker runtime client\n",
    "    sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "    try:\n",
    "        # Invoke the SageMaker endpoint\n",
    "        response = sagemaker_runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='text/csv',\n",
    "            Body=input_data_csv\n",
    "        )\n",
    "\n",
    "        # Parse the response body\n",
    "        result = json.loads(response['Body'].read().decode())\n",
    "        anomaly_scores = [score['score'] for score in result.get('scores', [])]\n",
    "        return anomaly_scores\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking the endpoint: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "# Replace 'your-endpoint-name' with the actual endpoint name\n",
    "example_input_data = test_inference\n",
    "\n",
    "endpoint_name = \"randomcutforest-2024-12-02-16-54-15-915\"\n",
    "anomaly_scores = invoke_rcf_endpoint(endpoint_name, example_input_data)\n",
    "\n",
    "if anomaly_scores:\n",
    "    print(\"Anomaly scores:\", anomaly_scores)\n",
    "else:\n",
    "    print(\"Failed to retrieve anomaly scores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fedcc1ca-f365-4152-9070-3b74c25caa04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: randomcutforest-2024-12-02-03-21-04-859\n",
      "INFO:sagemaker:Deleting endpoint with name: randomcutforest-2024-12-02-03-21-04-859\n"
     ]
    }
   ],
   "source": [
    "# Delete the endpoint (clean up)\n",
    "rcf_detector.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ba2a43-fcce-48c2-a9a9-1c5682aaf5ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Temp Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8aad04-2323-42ab-85a5-3f44e77a426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, ModelStep\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.sklearn.estimator import SKLearnEstimator\n",
    "from sagemaker.model import Model\n",
    "import boto3\n",
    "\n",
    "# Initialize the SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = 'your-bucket-name'  # Replace with your S3 bucket name\n",
    "prefix = 'sagemaker-pipeline'\n",
    "\n",
    "# Step 1: Data preprocessing (use your preprocessing script)\n",
    "script_processor = ScriptProcessor(\n",
    "    image_uri=sagemaker.image_uris.retrieve('sklearn', sagemaker_session.boto_region_name, version='0.23-1'),\n",
    "    role=role,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    command=['python3']\n",
    ")\n",
    "\n",
    "# Preprocessing step (your custom preprocessing function here)\n",
    "processing_step = ProcessingStep(\n",
    "    name=\"DataPreprocessingStep\",\n",
    "    processor=script_processor,\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            source=f's3://{bucket}/input_data/',\n",
    "            destination='/opt/ml/processing/input'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            source='/opt/ml/processing/output',\n",
    "            destination=f's3://{bucket}/preprocessed_data/'\n",
    "        )\n",
    "    ],\n",
    "    code='scripts/preprocess.py'  # Preprocessing script path\n",
    ")\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(\n",
    "    name=\"RandomCutForestPipeline\",\n",
    "    steps=[processing_step, training_step, model_step]\n",
    ")\n",
    "\n",
    "# Execute the pipeline\n",
    "pipeline.upsert(role_arn=role)\n",
    "pipeline.start()\n",
    "\n",
    "print(f\"Pipeline started: {pipeline.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4e8c8-6a12-4af7-a59f-f551f1109b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
    "print(\"RCF Results:\", results)\n",
    "\n",
    "# Step 3: Save results to S3\n",
    "s3_client.put_object(\n",
    "    Bucket=bucket_name,\n",
    "    Key=s3_key,\n",
    "    Body=json.dumps(results),\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "\n",
    "print(f\"Results saved to s3://{bucket_name}/{s3_key}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
