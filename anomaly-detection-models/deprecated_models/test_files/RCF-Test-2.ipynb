{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92dcc2a-7b0a-45cd-9fac-847da72c527e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n",
    "from sagemaker import Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d23235f-c951-489b-a390-515c028a41b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Original Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8da4225-e3c0-4edc-a78a-2acf9bae0a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name sagemaker_notebook_jupyter_lab to get Role path.\n"
     ]
    }
   ],
   "source": [
    "region_name = 'us-east-1'\n",
    "# Set the region using boto3\n",
    "boto3.setup_default_session(region_name=region_name)\n",
    "\n",
    "# Create a SageMaker session with the default boto3 session\n",
    "session = Session(boto_session=boto3.Session())\n",
    "\n",
    "# Get the execution role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26f4418-22d6-404f-bc67-3696c77a7eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify a different region, e.g., us-east-1\n",
    "# region = session.boto_region_name\n",
    "rcf_image = sagemaker.image_uris.retrieve('randomcutforest', region=region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45a1014f-6412-4200-9217-3d44ad5608fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare synthetic data to simulate traffic data\n",
    "data = pd.DataFrame({\n",
    "    'month': [1, 2, 3, 4, 5],\n",
    "    'week': [1, 2, 3, 4, 5],\n",
    "    'day': [1, 2, 3, 4, 5],\n",
    "    'season': [1, 2, 3, 4, 1],\n",
    "    'average_speed': [30, 45, 50, 60, 35],\n",
    "    'car_density': [5, 10, 15, 20, 8]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed9e499-d635-44e4-ac17-dc32a8fe1416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale average_speed and car_density\n",
    "scaler = MinMaxScaler()\n",
    "data[['average_speed', 'car_density']] = scaler.fit_transform(data[['average_speed', 'car_density']])\n",
    "\n",
    "# Optionally add cyclic encoding for time-based features\n",
    "data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
    "data['day_sin'] = np.sin(2 * np.pi * data['day'] / 7)\n",
    "data['day_cos'] = np.cos(2 * np.pi * data['day'] / 7)\n",
    "\n",
    "# Drop original columns and save preprocessed data to CSV\n",
    "preprocessed_data = data.drop(columns=['month', 'week', 'day', 'season'])\n",
    "preprocessed_data.to_csv('data.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "798445f0-9191-43b2-8fc4-bfd7585c7cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (AccessDenied) when calling the CreateBucket operation: User: arn:aws:sts::559050226112:assumed-role/sagemaker_notebook_jupyter_lab/SageMaker is not authorized to perform: s3:CreateBucket on resource: \"arn:aws:s3:::sagemaker-us-east-1-559050226112\" because no identity-based policy allows the s3:CreateBucket action",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:685\u001b[0m, in \u001b[0;36mSession.general_bucket_check_if_user_has_permission\u001b[0;34m(self, bucket_name, s3, bucket, region, bucket_creation_date_none)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 685\u001b[0m     \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_bucket\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (404) when calling the HeadBucket operation: Not Found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Upload data to S3\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m bucket \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_bucket\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrcf-example\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m s3 \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:602\u001b[0m, in \u001b[0;36mSession.default_bucket\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    599\u001b[0m     default_bucket \u001b[38;5;241m=\u001b[39m generate_default_sagemaker_bucket_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboto_session)\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_bucket_set_by_sdk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_s3_bucket_if_it_does_not_exist\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_bucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_bucket \u001b[38;5;241m=\u001b[39m default_bucket\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_bucket\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:634\u001b[0m, in \u001b[0;36mSession._create_s3_bucket_if_it_does_not_exist\u001b[0;34m(self, bucket_name, region)\u001b[0m\n\u001b[1;32m    632\u001b[0m bucket \u001b[38;5;241m=\u001b[39m s3\u001b[38;5;241m.\u001b[39mBucket(name\u001b[38;5;241m=\u001b[39mbucket_name)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bucket\u001b[38;5;241m.\u001b[39mcreation_date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 634\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneral_bucket_check_if_user_has_permission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_bucket_set_by_sdk:\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneral_bucket_check_if_user_has_permission(bucket_name, s3, bucket, region, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:692\u001b[0m, in \u001b[0;36mSession.general_bucket_check_if_user_has_permission\u001b[0;34m(self, bucket_name, s3, bucket, region, bucket_creation_date_none)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bucket_creation_date_none:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m404\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot Found\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 692\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_bucket_for_not_exist_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m403\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForbidden\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    694\u001b[0m         LOGGER\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m    695\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBucket \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m exists, but access is forbidden. Please try again after \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    696\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madding appropriate access.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    697\u001b[0m             bucket\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    698\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:716\u001b[0m, in \u001b[0;36mSession.create_bucket_for_not_exist_error\u001b[0;34m(self, bucket_name, region, s3)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m region \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus-east-1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    714\u001b[0m         \u001b[38;5;66;03m# 'us-east-1' cannot be specified because it is the default region:\u001b[39;00m\n\u001b[1;32m    715\u001b[0m         \u001b[38;5;66;03m# https://github.com/boto/boto3/issues/125\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m         \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_bucket\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    718\u001b[0m         s3\u001b[38;5;241m.\u001b[39mcreate_bucket(\n\u001b[1;32m    719\u001b[0m             Bucket\u001b[38;5;241m=\u001b[39mbucket_name,\n\u001b[1;32m    720\u001b[0m             CreateBucketConfiguration\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocationConstraint\u001b[39m\u001b[38;5;124m\"\u001b[39m: region},\n\u001b[1;32m    721\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/boto3/resources/factory.py:581\u001b[0m, in \u001b[0;36mResourceFactory._create_action.<locals>.do_action\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 581\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;66;03m# Clear cached data. It will be reloaded the next\u001b[39;00m\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;66;03m# time that an attribute is accessed.\u001b[39;00m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;66;03m# TODO: Make this configurable in the future?\u001b[39;00m\n\u001b[1;32m    587\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/boto3/resources/action.py:88\u001b[0m, in \u001b[0;36mServiceAction.__call__\u001b[0;34m(self, parent, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m     81\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     83\u001b[0m     parent\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mservice_name,\n\u001b[1;32m     84\u001b[0m     operation_name,\n\u001b[1;32m     85\u001b[0m     params,\n\u001b[1;32m     86\u001b[0m )\n\u001b[0;32m---> 88\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, response)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_handler(parent, params, response)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDenied) when calling the CreateBucket operation: User: arn:aws:sts::559050226112:assumed-role/sagemaker_notebook_jupyter_lab/SageMaker is not authorized to perform: s3:CreateBucket on resource: \"arn:aws:s3:::sagemaker-us-east-1-559050226112\" because no identity-based policy allows the s3:CreateBucket action"
     ]
    }
   ],
   "source": [
    "# Upload data to S3\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'rcf-example'\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file('data.csv', bucket, f'{prefix}/input/data.csv')\n",
    "input_data = f's3://{bucket}/{prefix}/input/data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24a4a626-ac2a-4d21-8f2e-1edc58dae77d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bucket' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m num_trees \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create the SageMaker estimator for RCF\u001b[39;00m\n\u001b[1;32m      6\u001b[0m rcf \u001b[38;5;241m=\u001b[39m sagemaker\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mEstimator(\n\u001b[1;32m      7\u001b[0m     image_uri\u001b[38;5;241m=\u001b[39mrcf_image,\n\u001b[1;32m      8\u001b[0m     role\u001b[38;5;241m=\u001b[39mrole,\n\u001b[1;32m      9\u001b[0m     instance_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     10\u001b[0m     instance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml.t3.medium\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Free Tier eligible\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbucket\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/output\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m     sagemaker_session\u001b[38;5;241m=\u001b[39msession\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m rcf\u001b[38;5;241m.\u001b[39mset_hyperparameters(num_samples_per_tree\u001b[38;5;241m=\u001b[39mnum_samples_per_tree, num_trees\u001b[38;5;241m=\u001b[39mnum_trees)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bucket' is not defined"
     ]
    }
   ],
   "source": [
    "# Configure the RCF model\n",
    "num_samples_per_tree = 256\n",
    "num_trees = 100\n",
    "\n",
    "# Create the SageMaker estimator for RCF\n",
    "rcf = sagemaker.estimator.Estimator(\n",
    "    image_uri=rcf_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.t3.medium',  # Free Tier eligible\n",
    "    output_path=f's3://{bucket}/{prefix}/output',\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "rcf.set_hyperparameters(num_samples_per_tree=num_samples_per_tree, num_trees=num_trees)\n",
    "\n",
    "# Train the model\n",
    "rcf.fit({'train': input_data})\n",
    "\n",
    "# Deploy the model to a SageMaker endpoint\n",
    "rcf_predictor = rcf.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t3.medium'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b5ce7-98e1-4874-896e-b40b1c901a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare synthetic data for inference\n",
    "inference_data = np.array([[0.5, 0.8, 0.3, 0.7]])  # Example normalized feature array\n",
    "payload = json.dumps(inference_data.tolist())\n",
    "\n",
    "# Make a prediction\n",
    "response = rcf_predictor.predict(payload)\n",
    "result = json.loads(response)\n",
    "print(\"Anomaly scores:\", result)\n",
    "\n",
    "# Clean up the endpoint\n",
    "rcf_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873bfbc-7ed8-4677-b40b-888495f73075",
   "metadata": {},
   "source": [
    "# Rewrote Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d25838c-d194-4545-8282-943721f3b72f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f3052bc-b92d-4f28-ab50-082bfaf75293",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name sagemaker_notebook_jupyter_lab to get Role path.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot format input    average_speed  car_density  month_sin     month_cos   day_sin   day_cos\n0       0.000000     0.000000   0.500000  8.660254e-01  0.781831  0.623490\n1       0.500000     0.333333   0.866025  5.000000e-01  0.974928 -0.222521\n2       0.666667     0.666667   1.000000  6.123234e-17  0.433884 -0.900969\n3       1.000000     1.000000   0.866025 -5.000000e-01 -0.433884 -0.900969\n4       0.166667     0.200000   0.500000 -8.660254e-01 -0.974928 -0.222521. Expecting one of str, TrainingInput, file_input or FileSystemInput",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 145\u001b[0m\n\u001b[1;32m    142\u001b[0m     result \u001b[38;5;241m=\u001b[39m deploy_and_predict(rcf_model, inference_data)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 145\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 136\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m local_file_path \u001b[38;5;241m=\u001b[39m save_preprocessed_data_to_local(preprocessed_data)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m rcf_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rcf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Inference data (example, replace with your own features)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m inference_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.7\u001b[39m]])  \u001b[38;5;66;03m# Example normalized feature array\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 83\u001b[0m, in \u001b[0;36mtrain_rcf_model\u001b[0;34m(input_data, output_data_file_path, region_name)\u001b[0m\n\u001b[1;32m     80\u001b[0m rcf\u001b[38;5;241m.\u001b[39mset_hyperparameters(num_samples_per_tree\u001b[38;5;241m=\u001b[39mnum_samples_per_tree, num_trees\u001b[38;5;241m=\u001b[39mnum_trees)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[43mrcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rcf\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/estimator.py:1369\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_for_training(job_name\u001b[38;5;241m=\u001b[39mjob_name)\n\u001b[1;32m   1368\u001b[0m experiment_config \u001b[38;5;241m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n\u001b[0;32m-> 1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job \u001b[38;5;241m=\u001b[39m \u001b[43m_TrainingJob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_new\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1371\u001b[0m forward_to_mlflow_tracking_server \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/estimator.py:2495\u001b[0m, in \u001b[0;36m_TrainingJob.start_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_new\u001b[39m(\u001b[38;5;28mcls\u001b[39m, estimator, inputs, experiment_config):\n\u001b[1;32m   2472\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a new Amazon SageMaker training job from the estimator.\u001b[39;00m\n\u001b[1;32m   2473\u001b[0m \n\u001b[1;32m   2474\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;124;03m        all information about the started training job.\u001b[39;00m\n\u001b[1;32m   2494\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2495\u001b[0m     train_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_train_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2497\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain args after processing defaults: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, train_args)\n\u001b[1;32m   2498\u001b[0m     estimator\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/estimator.py:2538\u001b[0m, in \u001b[0;36m_TrainingJob._get_train_args\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   2533\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_mode:\n\u001b[1;32m   2534\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2535\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile URIs are supported in local mode only. Please use a S3 URI instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2536\u001b[0m         )\n\u001b[0;32m-> 2538\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43m_Job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2540\u001b[0m current_hyperparameters \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mhyperparameters()\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_hyperparameters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/job.py:68\u001b[0m, in \u001b[0;36m_Job._load_config\u001b[0;34m(inputs, estimator, expand_role, validate_uri)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_config\u001b[39m(inputs, estimator, expand_role\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, validate_uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Placeholder docstring\"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     input_config \u001b[38;5;241m=\u001b[39m \u001b[43m_Job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_inputs_to_input_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_uri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     role \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     70\u001b[0m         estimator\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mexpand_role(estimator\u001b[38;5;241m.\u001b[39mrole)\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (expand_role \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_pipeline_variable(estimator\u001b[38;5;241m.\u001b[39mrole))\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m estimator\u001b[38;5;241m.\u001b[39mrole\n\u001b[1;32m     73\u001b[0m     )\n\u001b[1;32m     74\u001b[0m     output_config \u001b[38;5;241m=\u001b[39m _Job\u001b[38;5;241m.\u001b[39m_prepare_output_config(\n\u001b[1;32m     75\u001b[0m         estimator\u001b[38;5;241m.\u001b[39moutput_path,\n\u001b[1;32m     76\u001b[0m         estimator\u001b[38;5;241m.\u001b[39moutput_kms_key,\n\u001b[1;32m     77\u001b[0m         disable_output_compression\u001b[38;5;241m=\u001b[39mestimator\u001b[38;5;241m.\u001b[39mdisable_output_compression,\n\u001b[1;32m     78\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/job.py:142\u001b[0m, in \u001b[0;36m_Job._format_inputs_to_input_config\u001b[0;34m(inputs, validate_uri)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 142\u001b[0m         input_dict[k] \u001b[38;5;241m=\u001b[39m \u001b[43m_Job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_string_uri_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_uri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    144\u001b[0m     input_dict \u001b[38;5;241m=\u001b[39m _Job\u001b[38;5;241m.\u001b[39m_format_record_set_list_input(inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/job.py:200\u001b[0m, in \u001b[0;36m_Job._format_string_uri_input\u001b[0;34m(uri_input, validate_uri, content_type, input_mode, compression, target_attribute_name)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_pipeline_variable(uri_input):\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s3_input_result\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot format input \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Expecting one of str, TrainingInput, file_input or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFileSystemInput\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(uri_input)\n\u001b[1;32m    203\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot format input    average_speed  car_density  month_sin     month_cos   day_sin   day_cos\n0       0.000000     0.000000   0.500000  8.660254e-01  0.781831  0.623490\n1       0.500000     0.333333   0.866025  5.000000e-01  0.974928 -0.222521\n2       0.666667     0.666667   1.000000  6.123234e-17  0.433884 -0.900969\n3       1.000000     1.000000   0.866025 -5.000000e-01 -0.433884 -0.900969\n4       0.166667     0.200000   0.500000 -8.660254e-01 -0.974928 -0.222521. Expecting one of str, TrainingInput, file_input or FileSystemInput"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import Session\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Preprocessing Functions\n",
    "def preprocess_traffic_data(data):\n",
    "    \"\"\"\n",
    "    Preprocesses the traffic data by scaling and applying cyclic encoding for time-based features.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): Raw traffic data with columns ['month', 'week', 'day', 'season', 'average_speed', 'car_density'].\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Preprocessed traffic data with cyclic encoding and scaled features.\n",
    "    \"\"\"\n",
    "    # Scale average_speed and car_density\n",
    "    scaler = MinMaxScaler()\n",
    "    data[['average_speed', 'car_density']] = scaler.fit_transform(data[['average_speed', 'car_density']])\n",
    "\n",
    "    # Apply cyclic encoding for time-based features\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
    "    data['day_sin'] = np.sin(2 * np.pi * data['day'] / 7)\n",
    "    data['day_cos'] = np.cos(2 * np.pi * data['day'] / 7)\n",
    "\n",
    "    # Drop original columns and return preprocessed data\n",
    "    return data.drop(columns=['month', 'week', 'day', 'season'])\n",
    "\n",
    "def save_preprocessed_data_to_local(data, filename='data.csv'):\n",
    "    \"\"\"\n",
    "    Saves the preprocessed data to a local CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): Preprocessed traffic data.\n",
    "    - filename (str): The name of the local file to be saved (default is 'data.csv').\n",
    "    \"\"\"\n",
    "    # Save the preprocessed data to CSV\n",
    "    data.to_csv(filename, header=False, index=False)\n",
    "    return filename\n",
    "\n",
    "def upload_to_s3(local_file_path, bucket_name, s3_key):\n",
    "    \"\"\"\n",
    "    Uploads a local file to S3.\n",
    "\n",
    "    Parameters:\n",
    "    - local_file_path (str): The local file path of the CSV to be uploaded.\n",
    "    - bucket_name (str): The name of the S3 bucket.\n",
    "    - s3_key (str): The S3 object key (path where the file will be stored).\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_client.upload_file(local_file_path, bucket_name, s3_key)\n",
    "    \n",
    "    # Return the full S3 URI\n",
    "    return f\"s3://{bucket_name}/{s3_key}\"\n",
    "\n",
    "# SageMaker Training Function\n",
    "def train_rcf_model(input_data, output_data_file_path='output_data', region_name='us-east-1'):\n",
    "    \"\"\"\n",
    "    Trains the RCF model using the preprocessed data stored locally or in S3.\n",
    "\n",
    "    Parameters:\n",
    "    - input_data (str): Local file path or S3 URI of the preprocessed data.\n",
    "    - region_name (str): The AWS region for the SageMaker model.\n",
    "\n",
    "    Returns:\n",
    "    - sagemaker.estimator.Estimator: The trained RCF model.\n",
    "    \"\"\"\n",
    "    # Get the RCF image URI\n",
    "    rcf_image = sagemaker.image_uris.retrieve('randomcutforest', region=region_name)\n",
    "    \n",
    "    # Configure the RCF model\n",
    "    num_samples_per_tree = 256\n",
    "    num_trees = 100\n",
    "\n",
    "    # Get the execution role and session (assuming you're using SageMaker)\n",
    "    role = get_execution_role()\n",
    "    session = Session()\n",
    "\n",
    "    # Create SageMaker estimator for RCF\n",
    "    rcf = sagemaker.estimator.Estimator(\n",
    "        image_uri=rcf_image,\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        instance_type='ml.t3.medium',\n",
    "        output_path=f'{output_data_file_path}',  # Saving output locally\n",
    "        sagemaker_session=session\n",
    "    )\n",
    "\n",
    "    rcf.set_hyperparameters(num_samples_per_tree=num_samples_per_tree, num_trees=num_trees)\n",
    "\n",
    "    # Train the model\n",
    "    rcf.fit({'train': input_data})\n",
    "    \n",
    "    return rcf\n",
    "\n",
    "# Deployment & Prediction Function\n",
    "def deploy_and_predict(rcf_model, inference_data):\n",
    "    \"\"\"\n",
    "    Deploys the RCF model and makes a prediction.\n",
    "\n",
    "    Parameters:\n",
    "    - rcf_model (sagemaker.estimator.Estimator): The trained RCF model.\n",
    "    - inference_data (np.array): The data to be used for inference.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The predicted anomaly scores.\n",
    "    \"\"\"\n",
    "    # Deploy the model\n",
    "    rcf_predictor = rcf_model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.t3.medium'\n",
    "    )\n",
    "    \n",
    "    # Prepare the payload for inference\n",
    "    payload = json.dumps(inference_data.tolist())\n",
    "\n",
    "    # Make a prediction\n",
    "    response = rcf_predictor.predict(payload)\n",
    "    result = json.loads(response)\n",
    "    print(\"Anomaly scores:\", result)\n",
    "\n",
    "    # Clean up the endpoint\n",
    "    rcf_predictor.delete_endpoint()\n",
    "\n",
    "    return result\n",
    "\n",
    "# Main workflow\n",
    "def main(local_file=False, bucket_name=None, s3_key=None):\n",
    "    \"\"\"\n",
    "    Main workflow to process data, train the model, and make predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - local_file (bool): If True, the file is used locally; if False, the file is uploaded to S3.\n",
    "    - bucket_name (str): The name of the S3 bucket (used only if local_file is False).\n",
    "    - s3_key (str): The S3 key for the uploaded file (used only if local_file is False).\n",
    "    \"\"\"\n",
    "    # Sample data (replace with your actual preprocessed data)\n",
    "    data = pd.DataFrame({\n",
    "        'month': [1, 2, 3, 4, 5],\n",
    "        'week': [1, 2, 3, 4, 5],\n",
    "        'day': [1, 2, 3, 4, 5],\n",
    "        'season': [1, 2, 3, 4, 1],\n",
    "        'average_speed': [30, 45, 50, 60, 35],\n",
    "        'car_density': [5, 10, 15, 20, 8]\n",
    "    })\n",
    "\n",
    "    # Preprocess the data\n",
    "    preprocessed_data = preprocess_traffic_data(data)\n",
    "\n",
    "    # Save or upload data\n",
    "    if local_file:\n",
    "        local_file_path = save_preprocessed_data_to_local(preprocessed_data)\n",
    "        input_data = local_file_path\n",
    "    else:\n",
    "        local_file_path = save_preprocessed_data_to_local(preprocessed_data)  # Save locally first\n",
    "        input_data = upload_to_s3(local_file_path, bucket_name, s3_key)\n",
    "\n",
    "    # Train the model\n",
    "    rcf_model = train_rcf_model(input_data)\n",
    "\n",
    "    # Inference data (example, replace with your own features)\n",
    "    inference_data = np.array([[0.5, 0.8, 0.3, 0.7]])  # Example normalized feature array\n",
    "    \n",
    "    # Make a prediction\n",
    "    result = deploy_and_predict(rcf_model, inference_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Call main() with local_file=True to work locally or local_file=False for S3 workflow\n",
    "    main(local_file=False, bucket_name=\"your-s3-bucket-name\", s3_key=\"data/traffic_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
